# ğŸ§  Llama.cpp-Based AI Chat Assistant

This project is a lightweight, fully local AI assistant built using [llama.cpp](https://github.com/ggerganov/llama.cpp) and a quantized Qwen1.5 0.5B GGUF model. It runs completely offline on my local machine using WSL (Ubuntu on Windows 10) â€” no internet or cloud required.

## âœ¨ Features

- ğŸ§  Uses **Qwen1.5-0.5B-Chat** model in `GGUF` format
- âš¡ Runs on **CPU** with no GPU required
- ğŸ’» Built using `llama.cpp` with full CMake build system
- ğŸª¶ Lightweight: ~4GB RAM usage with quantized Q4_K_M model
- ğŸŒ Works entirely **offline** after download
- ğŸ’¬ Interactive CLI with conversation-style responses
- ğŸ§° Beginner-friendly â€” no prior ML experience needed

------

## âœ¨ What It Does

- Interactively chats like a personal assistant using a local LLM (Qwen1.5-0.5B GGUF)
- Processes user prompts in real-time via command line
- Runs efficiently on low-end hardware (8GB RAM / no GPU)
- Uses [llama.cpp](https://github.com/ggerganov/llama.cpp), a C++ inference engine optimized for speed and low memory

---

## ğŸ”§ Tech Stack

- ğŸ’» [llama.cpp](https://github.com/ggerganov/llama.cpp)
- ğŸ§  Qwen1.5-0.5B-Chat-GGUF quantized model (Q4_K_M)
- ğŸ§ WSL (Ubuntu 22.04) on Windows 10
- ğŸ™ Git, CMake, and g++ (GCC) for compilation
- ğŸ“¦ Model downloaded from Hugging Face

---

## ğŸ“ Project Structure

llama-ai-assistant/
â”œâ”€â”€ models/ # GGUF model goes here
â”‚ â””â”€â”€ qwen1_5-0_5b-chat-q4_k_m.gguf
â”œâ”€â”€ build/ # llama.cpp build output
â”‚ â””â”€â”€ main # Compiled binary
â”œâ”€â”€ README.md # You're reading it
â””â”€â”€ llama.cpp/ # Source code (cloned from upstream)
---

## ğŸš€ How to Run

1. Clone [llama.cpp](https://github.com/ggerganov/llama.cpp) and build:
    ```bash
    git clone https://github.com/ggerganov/llama.cpp.git
    cd llama.cpp && mkdir build && cd build
    cmake .. && make
    ```
2. Download the quantized model (Qwen1.5 0.5B - Q4_K_M):
    ```bash
    huggingface-cli login
    huggingface-cli download TheBloke/Qwen1.5-0.5B-Chat-GGUF \
      qwen1_5-0_5b-chat-q4_k_m.gguf \
      --local-dir ../models
    ```

3. Run the assistant:
    ```bash
    ./main -m ../models/qwen1_5-0_5b-chat-q4_k_m.gguf -i -n 512 --color
    ```

---

## ğŸ“ What I Learned

- How to compile and run `llama.cpp` in WSL on limited hardware
- How to download and run GGUF models offline
- Basics of memory-efficient model quantization (Q4_K_M)
- Model inference using CPU-only backends
- Structuring, documenting, and showcasing ML tools on GitHub

---

## âœ… Use Cases

- Learning LLM internals and prompt-response mechanics
- Offline chatbot with no API or cloud dependency
- Foundation for building local voice/chat assistants

---

## ğŸ™‹â€â™€ï¸ Credits

- Model by [TheBloke on HuggingFace](https://huggingface.co/TheBloke)
- llama.cpp by [ggerganov](https://github.com/ggerganov/llama.cpp)

---

## ğŸ“œ License

This project follows the licenses of the respective model and llama.cpp (MIT).

---


## Demo Screenshot ğŸ“¸

![Screenshot of AI Assistant](screenshots/ai-screenshot.jfif)

## Demo Video ğŸ¥

[Click to Watch Demo](videos/ai-assistant-demo.mp4)

